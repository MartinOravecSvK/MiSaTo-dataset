{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiSaTo-Dataset: a tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show how our QM and MD dataset are stored in h5 files. We also show how the data can be loaded so that it can be used by a deep learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the useful packages and set up the paths of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/micromamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'transforms.RandomTranslate' is deprecated, use 'transforms.RandomJitter' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np \n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from data.components.datasets import MolDataset, ProtDataset\n",
    "from data.components.transformQM import GNNTransformQM\n",
    "from data.components.transformMD import GNNTransformMD\n",
    "from data.qm_datamodule import QMDataModule\n",
    "from data.md_datamodule import MDDataModule\n",
    "from data.processing import preprocessing_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmh5_file = \"../data/QM/h5_files/tiny_qm.hdf5\"\n",
    "norm_file = \"../data/QM/h5_files/qm_norm.hdf5\"\n",
    "norm_txtfile = \"../data/QM/splits/train_norm.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H5 files presentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the QM H5 file and H5 file used to normalize the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm_H5File = h5py.File(qmh5_file)\n",
    "qm_normFile = h5py.File(norm_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ligands can be accessed using the pdb-id. Bellow we show the first ten molecules of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['10gs', '11gs', '13gs', '16pk', '184l', '185l', '186l', '187l', '188l', '1a07', '1a08', '1a09', '1a0q', '1a0tA', '1a0tB', '1a1b', '1a1c', '1a1e', '1a28', '1a2c', '1a30', '1a37', '1a42', '1a46', '1a4g', '1a4h', '1a4k', '1a4m', '1a4q', '1a4r', '1a4w', '1a50', '1a52', '1a5g', '1a5h', '1a5v', '1a61', '1a69', '1a7c', '1a7t', '1a7x', '1a85', '1a86', '1a8i', '1a8t', '1a94', '1a99', '1a9m', '1a9q', '1a9u']>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm_H5File.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access to the molecule trajectories as follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = qm_H5File[\"10gs\"][\"atom_properties\"][\"atom_properties_values\"][:, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target values can be accessed by specifiying into bracket the molecule name, then mol_properties and finally the name of the target value that we want to access: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0974"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm_H5File[\"10gs\"][\"mol_properties\"][\"Electron_Affinity\"][()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access to the mean and standard-deviation of each target value by specifiying it into bracket.\n",
    "We first specify the set, then the target value and finally either mean or std. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['Electron_Affinity', 'Electronegativity', 'Hardness', 'Ionization_Potential']>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm_normFile.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.33265\n",
      "18.636927\n"
     ]
    }
   ],
   "source": [
    "print(qm_normFile[\"Electron_Affinity\"][\"mean\"][()])\n",
    "print(qm_normFile[\"Electron_Affinity\"][\"std\"][()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QM and MD datasets are warped into a PyTorch Dataset class under the name MolDataset and ProtDataset, respectively. \n",
    "The parameters taken by the two classes as well as their types can be found as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MolDataset in module data.components.datasets:\n",
      "\n",
      "class MolDataset(torch.utils.data.dataset.Dataset)\n",
      " |  MolDataset(data_file, idx_file, target_norm_file, transform, isTrain=False, post_transform=None)\n",
      " |  \n",
      " |  Load the QM dataset.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MolDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, index: int)\n",
      " |  \n",
      " |  __init__(self, data_file, idx_file, target_norm_file, transform, isTrain=False, post_transform=None)\n",
      " |      Args:\n",
      " |          data_file (str): H5 file path\n",
      " |          idx_file (str): path of txt file which contains pdb ids for a specific split such as train, val or test.\n",
      " |          target_norm_file (str): H5 file path where training mean and std are stored.  \n",
      " |          transform (obj): class that convert a dict to a PyTorch Geometric graph.\n",
      " |          isTrain (bool, optional): Flag to standardize the target values (only used for train set). Defaults to False.\n",
      " |          post_transform (PyTorch Geometric, optional): data augmentation. Defaults to None.\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MolDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ProtDataset in module data.components.datasets:\n",
      "\n",
      "class ProtDataset(torch.utils.data.dataset.Dataset)\n",
      " |  ProtDataset(md_data_file, idx_file, transform=None, post_transform=None)\n",
      " |  \n",
      " |  Load the MD dataset\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ProtDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, index: int)\n",
      " |  \n",
      " |  __init__(self, md_data_file, idx_file, transform=None, post_transform=None)\n",
      " |      Args:\n",
      " |          md_data_file (str): H5 file path\n",
      " |          idx_file (str): path of txt file which contains pdb ids for a specific split such as train, val or test.\n",
      " |          transform (obj): class that convert a dict to a PyTorch Geometric graph.\n",
      " |          post_transform (PyTorch Geometric, optional): data augmentation. Defaults to None.\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ProtDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the data by instanciating MolDataset and providing the QM H5 file, the text file that indicates the molecule used for training and the norm file used to normalize the target values. \n",
    "\n",
    "The MolDataset class without any transform return a dictionary that contain the elements and their coordinates. We use GNNTransformQM class to transform our data to a graph that can be used by a GNN. The parameter post_transform is another transformation used to perform data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = \"../data/QM/splits/train_tinyQM.txt\"\n",
    "\n",
    "transform = T.RandomTranslate(0.25)\n",
    "batch_size = 128\n",
    "num_workers = 48\n",
    "\n",
    "data_train = MolDataset(qmh5_file, train, target_norm_file=norm_file, transform=GNNTransformQM(), post_transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can load our data using the PyTorch DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[1602, 25], edge_index=[2, 30354], edge_attr=[30354, 1], y=[60], pos=[1602, 3], id=[30], batch=[1602], ptr=[31])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(data_train, batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "for idx, val in enumerate(train_loader):\n",
    "    print(val)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch lightning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QMDataModule is a class inherated from LightningDataModule that instanciate the MolDataset for training, validation and test set and returns a dataloader for each set. \n",
    "\n",
    "We start by instanciation of the QMDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_root =  \"../data/QM\"\n",
    "\n",
    "qmh5file = \"h5_files/tiny_qm.hdf5\"\n",
    "\n",
    "tr = \"splits/train_tinyQM.txt\"\n",
    "v = \"splits/val_tinyQM.txt\"\n",
    "te = \"splits/test_tinyQM.txt\"\n",
    "\n",
    "qmdata = QMDataModule(files_root, h5file=qmh5file, train=tr, val=v, test=te, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we call the setup function to instanciate the MolDataset for training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmdata.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can return a dataloader for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[1602, 25], edge_index=[2, 30354], edge_attr=[30354, 1], y=[60], pos=[1602, 3], id=[30], batch=[1602], ptr=[31])\n"
     ]
    }
   ],
   "source": [
    "train_loader = qmdata.train_dataloader()\n",
    "\n",
    "for idx, val in enumerate(train_loader):\n",
    "    print(val)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generated a tiny h5 file that can be inspected right away. We do this for the structure with pdb-id 10GS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdh5_file_tiny = '../data/MD/h5_files/tiny_md.hdf5'\n",
    "md_H5File_tiny = h5py.File(mdh5_file_tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['atoms_element', 'atoms_number', 'atoms_residue', 'atoms_type', 'frames_bSASA', 'frames_distance', 'frames_interaction_energy', 'frames_rmsd_ligand', 'molecules_begin_atom_index', 'trajectory_coordinates']>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_H5File_tiny['10GS'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beginning of the name of each property indicates the respective shape:\n",
    "- atoms_ have a entry for each atom of the structure\n",
    "- frames_ have an entry for each of the 100 frames\n",
    "- molecules_ has an entry for each molecule, including the ligand\n",
    "- trajectory_coordinates_ has an entry of each atom and each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('atoms_element', (6593,)),\n",
       " ('atoms_number', (6593,)),\n",
       " ('atoms_residue', (6593,)),\n",
       " ('atoms_type', (6593,)),\n",
       " ('frames_bSASA', (100,)),\n",
       " ('frames_distance', (100,)),\n",
       " ('frames_interaction_energy', (100,)),\n",
       " ('frames_rmsd_ligand', (100,)),\n",
       " ('molecules_begin_atom_index', (3,)),\n",
       " ('trajectory_coordinates', (100, 6593, 3))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(key, np.shape(md_H5File_tiny['10GS'][key])) for key in md_H5File_tiny['10GS'].keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run models for the MD dataset you will most likely need to preprocess the h5 file based on your model. We provide a preprocessing script (see data/processing/preprocessing_db.py) that can filter out the atom types that you are not interested in (e.g. H-atoms) or calculate values of interest based on your models.\n",
    "Here, we will show how to use the script to calculate the adaptability values on the dataset and stripping the H-atoms.  \n",
    "In this notebook we define a new Args class, if you use the script in the terminal just provide these values as input parameters in the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/MD/h5_files/tiny_md_out.hdf5\n",
      "Removing existing output file...\n",
      "10GS 1\n",
      "Stripping  atoms_element 1  and calculating adaptability for the atoms that were not stripped.\n",
      "11GS 2\n",
      "13GS 3\n",
      "16PK 4\n",
      "184L 5\n",
      "185L 6\n",
      "186L 7\n",
      "187L 8\n",
      "188L 9\n",
      "1A07 10\n",
      "1A08 11\n",
      "1A09 12\n",
      "1A0Q 13\n",
      "1A1B 14\n",
      "1A1C 15\n",
      "1A1E 16\n",
      "1A28 17\n",
      "1A2C 18\n",
      "1A30 19\n",
      "1A3E 20\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    # input file\n",
    "    datasetIn = \"../data/MD/h5_files/tiny_md.hdf5\"\n",
    "    # Feature that should be stripped, e.g. atoms_element or atoms_type\n",
    "    strip_feature = \"atoms_element\"\n",
    "    # Value to strip, e.g. if strip_freature= atoms_element; 1 for H. \n",
    "    strip_value = 1\n",
    "    # Start index of structures\n",
    "    begin = 0\n",
    "    # End index of structures\n",
    "    end = 20 \n",
    "    # We calculate the adaptability for each atom. \n",
    "    # Default behaviour will also strip H atoms, if no stripping should be perfomed set strip_value to -1.\n",
    "    Adaptability = True\n",
    "    # If set to True this will create a new feature that combines one entry for each protein AA but all ligand entries; \n",
    "    # e.g. for only ca set strip_feature = atoms_type and strip_value = 14\n",
    "    Pres_Lat = False\n",
    "    # We strip the complex by given distance (in Angstrom) from COG of molecule, \n",
    "    # use e.g. 15.0. If default value is given (0.0) no pocket stripping will be applied.\n",
    "    Pocket = 0.0\n",
    "    # output file name and location\n",
    "    datasetOut = \"../data/MD/h5_files/tiny_md_out.hdf5\"\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "preprocessing_db.main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same steps used for QM can be used to load the MD dataset. We start by loading the generated h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_root =  \"\"\n",
    "\n",
    "mdh5_file = '../data/MD/h5_files/tiny_md_out.hdf5'\n",
    "\n",
    "train_idx = \"../data/MD/splits/train_tinyMD.txt\"\n",
    "val_idx = \"../data/MD/splits/val_tinyMD.txt\"\n",
    "test_idx = \"../data/MD/splits/test_tinyMD.txt\"\n",
    "\n",
    "md_H5File = h5py.File(mdh5_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During preprocessing the H-atoms were stripped (see the change in atoms_ shape) and a new feature, the adaptability was calculated for each atom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('atoms_coordinates_ref', (3295, 3)),\n",
       " ('atoms_element', (3295,)),\n",
       " ('atoms_number', (3295,)),\n",
       " ('atoms_residue', (3295,)),\n",
       " ('atoms_type', (3295,)),\n",
       " ('feature_atoms_adaptability', (3295,)),\n",
       " ('frames_bSASA', (100,)),\n",
       " ('frames_distance', (100,)),\n",
       " ('frames_interaction_energy', (100,)),\n",
       " ('frames_rmsd_ligand', (100,)),\n",
       " ('molecules_begin_atom_index', (3,)),\n",
       " ('trajectory_coordinates', (100, 3295, 3))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(key, np.shape(md_H5File['10GS'][key])) for key in md_H5File['10GS'].keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atom's coordinates from the first frame \n",
    "xyz = md_H5File['10GS']['trajectory_coordinates'][0, :, :] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initiate the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ProtDataset(mdh5_file, idx_file=train_idx, transform=GNNTransformMD(), post_transform=T.RandomTranslate(0.05))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, num_workers=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/user/micromamba/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/user/micromamba/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/user/micromamba/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/tillsiebenmorgen/Projects/MiSaTo-dataset/src/data/components/datasets.py\", line 57, in __getitem__\n    item[\"scores\"] = pitem[\"atoms_soft_hard\"][:][:cutoff]\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"/home/user/micromamba/lib/python3.10/site-packages/h5py/_hl/group.py\", line 357, in __getitem__\n    oid = h5o.open(self.id, self._e(name), lapl=self._lapl)\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py/h5o.pyx\", line 190, in h5py.h5o.open\nKeyError: \"Unable to open object (object 'atoms_soft_hard' doesn't exist)\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(val)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/home/user/micromamba/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/home/user/micromamba/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/user/micromamba/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/home/user/micromamba/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/user/micromamba/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/user/micromamba/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/user/micromamba/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/tillsiebenmorgen/Projects/MiSaTo-dataset/src/data/components/datasets.py\", line 57, in __getitem__\n    item[\"scores\"] = pitem[\"atoms_soft_hard\"][:][:cutoff]\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"/home/user/micromamba/lib/python3.10/site-packages/h5py/_hl/group.py\", line 357, in __getitem__\n    oid = h5o.open(self.id, self._e(name), lapl=self._lapl)\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py/h5o.pyx\", line 190, in h5py.h5o.open\nKeyError: \"Unable to open object (object 'atoms_soft_hard' doesn't exist)\"\n"
     ]
    }
   ],
   "source": [
    "for idx, val in enumerate(train_loader):\n",
    "    print(val)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mddata = MDDataModule(files_root, h5file=mdh5_file, train=train_idx, val=val_idx, test=test_idx, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mddata.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[38378, 11], edge_index=[2, 622332], edge_attr=[622332], y=[38378], pos=[38378, 3], ids=[16], batch=[38378], ptr=[17])\n"
     ]
    }
   ],
   "source": [
    "train_loader = mddata.train_dataloader()\n",
    "\n",
    "for idx, val in enumerate(train_loader):\n",
    "    print(val)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ccb8a31603cded50fa2c46f04b1e1d1666ffae07322ba5b5b2be24a82f8ce498"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
