{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiSaTo-Dataset: a tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show how our QM and MD dataset are stored in h5 files. We also show how the data can be loaded so that it can be used by a deep learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the useful packages and set up the paths of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/micromamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'transforms.RandomTranslate' is deprecated, use 'transforms.RandomJitter' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np \n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from data.components.datasets import MolDataset, ProtDataset\n",
    "from data.components.transformQM import GNNTransformQM\n",
    "from data.components.transformMD import GNNTransformMD\n",
    "from data.qm_datamodule import QMDataModule\n",
    "from data.md_datamodule import MDDataModule\n",
    "from data.processing import preprocessing_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmh5_file = \"../data/QM/h5_files/tiny_qm.hdf5\"\n",
    "norm_file = \"../data/QM/h5_files/qm_norm.hdf5\"\n",
    "norm_txtfile = \"../data/QM/splits/train_norm.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H5 files presentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the QM H5 file and H5 file used to normalize the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm_H5File = h5py.File(qmh5_file)\n",
    "qm_normFile = h5py.File(norm_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ligands can be accessed using the pdb-id. Bellow we show the first ten molecules of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['10gs', '11gs', '13gs', '16pk', '184l', '185l', '186l', '187l', '188l', '1a07', '1a08', '1a09', '1a0q', '1a0tA', '1a0tB', '1a1b', '1a1c', '1a1e', '1a28', '1a2c', '1a30', '1a37', '1a42', '1a46', '1a4g', '1a4h', '1a4k', '1a4m', '1a4q', '1a4r', '1a4w', '1a50', '1a52', '1a5g', '1a5h', '1a5v', '1a61', '1a69', '1a7c', '1a7t', '1a7x', '1a85', '1a86', '1a8i', '1a8t', '1a94', '1a99', '1a9m', '1a9q', '1a9u']>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm_H5File.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access to the molecule trajectories as follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = qm_H5File[\"10gs\"][\"atom_properties\"][\"atom_properties_values\"][:, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target values can be accessed by specifiying into bracket the molecule name, then mol_properties and finally the name of the target value that we want to access: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0974"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm_H5File[\"10gs\"][\"mol_properties\"][\"Electron_Affinity\"][()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access to the mean and standard-deviation of each target value by specifiying it into bracket.\n",
    "We first specify the set, then the target value and finally either mean or std. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['Electron_Affinity', 'Electronegativity', 'Hardness', 'Ionization_Potential']>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm_normFile.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.33265\n",
      "18.636927\n"
     ]
    }
   ],
   "source": [
    "print(qm_normFile[\"Electron_Affinity\"][\"mean\"][()])\n",
    "print(qm_normFile[\"Electron_Affinity\"][\"std\"][()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QM and MD datasets are warped into a PyTorch Dataset class under the name MolDataset and ProtDataset, respectively. \n",
    "The parameters taken by the two classes as well as their types can be found as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MolDataset in module data.components.datasets:\n",
      "\n",
      "class MolDataset(torch.utils.data.dataset.Dataset)\n",
      " |  MolDataset(data_file, idx_file, target_norm_file, transform, isTrain=False, post_transform=None)\n",
      " |  \n",
      " |  Load the QM dataset.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MolDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, index: int)\n",
      " |  \n",
      " |  __init__(self, data_file, idx_file, target_norm_file, transform, isTrain=False, post_transform=None)\n",
      " |      Args:\n",
      " |          data_file (str): H5 file path\n",
      " |          idx_file (str): path of txt file which contains pdb ids for a specific split such as train, val or test.\n",
      " |          target_norm_file (str): H5 file path where training mean and std are stored.  \n",
      " |          transform (obj): class that convert a dict to a PyTorch Geometric graph.\n",
      " |          isTrain (bool, optional): Flag to standardize the target values (only used for train set). Defaults to False.\n",
      " |          post_transform (PyTorch Geometric, optional): data augmentation. Defaults to None.\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MolDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ProtDataset in module data.components.datasets:\n",
      "\n",
      "class ProtDataset(torch.utils.data.dataset.Dataset)\n",
      " |  ProtDataset(md_data_file, idx_file, transform=None, post_transform=None)\n",
      " |  \n",
      " |  Load the MD dataset\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ProtDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, index: int)\n",
      " |  \n",
      " |  __init__(self, md_data_file, idx_file, transform=None, post_transform=None)\n",
      " |      Args:\n",
      " |          md_data_file (str): H5 file path\n",
      " |          idx_file (str): path of txt file which contains pdb ids for a specific split such as train, val or test.\n",
      " |          transform (obj): class that convert a dict to a PyTorch Geometric graph.\n",
      " |          post_transform (PyTorch Geometric, optional): data augmentation. Defaults to None.\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ProtDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the data by instanciating MolDataset and providing the QM H5 file, the text file that indicates the molecule used for training and the norm file used to normalize the target values. \n",
    "\n",
    "The MolDataset class without any transform return a dictionary that contain the elements and their coordinates. We use GNNTransformQM class to transform our data to a graph that can be used by a GNN. The parameter post_transform is another transformation used to perform data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = \"../data/QM/splits/train_tinyQM.txt\"\n",
    "\n",
    "transform = T.RandomTranslate(0.25)\n",
    "batch_size = 128\n",
    "num_workers = 48\n",
    "\n",
    "data_train = MolDataset(qmh5_file, train, target_norm_file=norm_file, transform=GNNTransformQM(), post_transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can load our data using the PyTorch DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[1602, 25], edge_index=[2, 30354], edge_attr=[30354, 1], y=[60], pos=[1602, 3], id=[30], batch=[1602], ptr=[31])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(data_train, batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "for idx, val in enumerate(train_loader):\n",
    "    print(val)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch lightning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QMDataModule is a class inherated from LightningDataModule that instanciate the MolDataset for training, validation and test set and returns a dataloader for each set. \n",
    "\n",
    "We start by instanciation of the QMDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_root =  \"../data/QM\"\n",
    "\n",
    "qmh5file = \"h5_files/tiny_qm.hdf5\"\n",
    "\n",
    "tr = \"splits/train_tinyQM.txt\"\n",
    "v = \"splits/val_tinyQM.txt\"\n",
    "te = \"splits/test_tinyQM.txt\"\n",
    "\n",
    "qmdata = QMDataModule(files_root, h5file=qmh5file, train=tr, val=v, test=te, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we call the setup function to instanciate the MolDataset for training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmdata.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can return a dataloader for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[1602, 25], edge_index=[2, 30354], edge_attr=[30354, 1], y=[60], pos=[1602, 3], id=[30], batch=[1602], ptr=[31])\n"
     ]
    }
   ],
   "source": [
    "train_loader = qmdata.train_dataloader()\n",
    "\n",
    "for idx, val in enumerate(train_loader):\n",
    "    print(val)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the hdf5 file for the adaptability predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args <__main__.Args object at 0x7f50886f2260>\n",
      "elementMap {'2C': 1, '3C': 2, 'C': 3, 'C*': 4, 'C8': 5, 'CA': 6, 'CB': 7, 'CC': 8, 'CN': 9, 'CO': 10, 'CR': 11, 'CT': 12, 'CW': 13, 'CX': 14, 'H': 15, 'H1': 16, 'H4': 17, 'H5': 18, 'HA': 19, 'HC': 20, 'HO': 21, 'HP': 22, 'HS': 23, 'N': 24, 'N2': 25, 'N3': 26, 'NA': 27, 'NB': 28, 'O': 29, 'O2': 30, 'OH': 31, 'S': 32, 'SH': 33, 'br': 34, 'c': 35, 'c1': 36, 'c2': 37, 'c3': 38, 'ca': 39, 'cc': 40, 'cd': 41, 'ce': 42, 'cf': 43, 'cg': 44, 'ch': 45, 'cl': 46, 'cp': 47, 'cq': 48, 'cs': 49, 'cu': 50, 'cx': 51, 'cy': 52, 'cz': 53, 'f': 54, 'h1': 55, 'h2': 56, 'h3': 57, 'h4': 58, 'h5': 59, 'ha': 60, 'hc': 61, 'hn': 62, 'ho': 63, 'hp': 64, 'hs': 65, 'hx': 66, 'i': 67, 'n': 68, 'n1': 69, 'n2': 70, 'n3': 71, 'n4': 72, 'n7': 73, 'n8': 74, 'na': 75, 'nb': 76, 'nc': 77, 'nd': 78, 'ne': 79, 'nf': 80, 'nh': 81, 'ni': 82, 'nj': 83, 'nk': 84, 'nl': 85, 'nm': 86, 'nn': 87, 'no': 88, 'nq': 89, 'ns': 90, 'nt': 91, 'nu': 92, 'nv': 93, 'nx': 94, 'ny': 95, 'nz': 96, 'o': 97, 'oh': 98, 'op': 99, 'oq': 100, 'os': 101, 'p5': 102, 'py': 103, 's': 104, 's4': 105, 's6': 106, 'sh': 107, 'ss': 108, 'sx': 109, 'sy': 110}\n",
      "10gs 1\n",
      "{'trajectory_coordinates': None, 'atoms_type': <HDF5 dataset \"atoms_type\": shape (3295,), type \"<i8\">, 'atoms_number': <HDF5 dataset \"atoms_number\": shape (3295,), type \"<i8\">, 'atoms_residue': <HDF5 dataset \"atoms_residue\": shape (3295,), type \"<i8\">, 'atoms_element': <HDF5 dataset \"atoms_element\": shape (3295,), type \"<i8\">, 'molecules_begin_atom_index': <HDF5 dataset \"molecules_begin_atom_index\": shape (3,), type \"<i8\">, 'frames_rmsd_ligand': None, 'frames_distance': None, 'frames_interaction_energy': None, 'frames_bSASA': None}\n",
      "Stripping  atoms_element 1  and calculating adaptability for the atoms that were not stripped.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 27\u001b[0m\n\u001b[1;32m     22\u001b[0m     datasetOut \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/MD/h5_files/tiny_md_out.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m args \u001b[38;5;241m=\u001b[39m Args()\n\u001b[0;32m---> 27\u001b[0m \u001b[43mpreprocessing_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/MiSaTo-dataset/src/data/processing/preprocessing_db.py:247\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStripping \u001b[39m\u001b[38;5;124m'\u001b[39m, args\u001b[38;5;241m.\u001b[39mstrip_feature, args\u001b[38;5;241m.\u001b[39mstrip_value, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and calculating adaptability for the atoms that were not stripped.\u001b[39m\u001b[38;5;124m'\u001b[39m)                \n\u001b[0;32m--> 247\u001b[0m preprocessing_entries \u001b[38;5;241m=\u001b[39m \u001b[43mstrip_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrip_properties\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5_entries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip_feature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m preprocessing_entries[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_atoms_adaptability\u001b[39m\u001b[38;5;124m\"\u001b[39m], preprocessing_entries[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_atoms_adaptability\u001b[39m\u001b[38;5;124m\"\u001b[39m], preprocessing_entries[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124matoms_coordinates_ref\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m adaptability(preprocessing_entries)\n\u001b[1;32m    249\u001b[0m preprocessing_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/MiSaTo-dataset/src/data/processing/preprocessing_db.py:112\u001b[0m, in \u001b[0;36mstrip_feature\u001b[0;34m(args, strip_properties, h5_entries, strip_value, strip_feature, inversion)\u001b[0m\n\u001b[1;32m    110\u001b[0m     stripped_entries[strip_property] \u001b[38;5;241m=\u001b[39m h5_entries[strip_property][stripped_indices]\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strip_property\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrajectory_\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 112\u001b[0m     stripped_entries[strip_property] \u001b[38;5;241m=\u001b[39m \u001b[43mh5_entries\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstrip_property\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstripped_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strip_property\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmolecules_\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    114\u001b[0m     stripped_entries[strip_property] \u001b[38;5;241m=\u001b[39m stripped_molecules_begin_atom_index\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    # input file\n",
    "    datasetIn = \"../data/MD/h5_files/tiny_md.hdf5\"\n",
    "    # Feature that should be stripped, e.g. atoms_element or atoms_type\n",
    "    strip_feature = \"atoms_element\"\n",
    "    # Value to strip, e.g. if strip_freature= atoms_element; 1 for H. \n",
    "    strip_value = 1\n",
    "    # Start index of structures\n",
    "    begin = 0\n",
    "    # End index of structures\n",
    "    end = 50 \n",
    "    # We calculate the adaptability for each atom. \n",
    "    # Default behaviour will also strip H atoms, if no stripping should be perfomed set strip_value to -1.\n",
    "    Adaptability = True\n",
    "    # If set to True this will create a new feature that combines one entry for each protein AA but all ligand entries; \n",
    "    # e.g. for only ca set strip_feature = atoms_type and strip_value = 14\n",
    "    Pres_Lat = False\n",
    "    # We strip the complex by given distance (in Angstrom) from COG of molecule, \n",
    "    # use e.g. 15.0. If default value is given (0.0) no pocket stripping will be applied.\n",
    "    Pocket = 0.0\n",
    "    # output file name and location\n",
    "    datasetOut = \"../data/MD/h5_files/tiny_md_out.hdf5\"\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "preprocessing_db.main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same steps used for MD can be used to load the MD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = '../data/MD/h5_files/tiny_md_out.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m val_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/MD/splits/val_tinyMD.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m test_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/MD/splits/test_tinyMD.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m md_H5File \u001b[38;5;241m=\u001b[39m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmdh5_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/user/micromamba/lib/python3.10/site-packages/h5py/_hl/files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    558\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    560\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    561\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    562\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    563\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    564\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    565\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    566\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 567\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/home/user/micromamba/lib/python3.10/site-packages/h5py/_hl/files.py:231\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    230\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 231\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    233\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '../data/MD/h5_files/tiny_md_out.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "files_root =  \"\"\n",
    "\n",
    "mdh5_file = '../data/MD/h5_files/tiny_md_out.hdf5'\n",
    "\n",
    "train_idx = \"../data/MD/splits/train_tinyMD.txt\"\n",
    "val_idx = \"../data/MD/splits/val_tinyMD.txt\"\n",
    "test_idx = \"../data/MD/splits/test_tinyMD.txt\"\n",
    "\n",
    "md_H5File = h5py.File(mdh5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_root =  \"\"\n",
    "\n",
    "mdh5_file = '../data/MD/h5_files/tiny_md.hdf5'\n",
    "\n",
    "train_idx = \"../data/MD/splits/train_tinyMD.txt\"\n",
    "val_idx = \"../data/MD/splits/val_tinyMD.txt\"\n",
    "test_idx = \"../data/MD/splits/test_tinyMD.txt\"\n",
    "\n",
    "md_H5File = h5py.File(mdh5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atom's coordinates from the first frame \n",
    "xyz = md_H5File.keys()\n",
    "len(xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unable to open object (object 'trajectory_coordinates' doesn't exist)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Atom's coordinates from the first frame \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m xyz \u001b[38;5;241m=\u001b[39m \u001b[43mmd_H5File\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m13gs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrajectory_coordinates\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m, :, :] \n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/home/user/micromamba/lib/python3.10/site-packages/h5py/_hl/group.py:357\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:190\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unable to open object (object 'trajectory_coordinates' doesn't exist)\""
     ]
    }
   ],
   "source": [
    "# Atom's coordinates from the first frame \n",
    "xyz = md_H5File['13gs']['trajectory_coordinates'][0, :, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mdh5_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m ProtDataset(\u001b[43mmdh5_file\u001b[49m, idx_file\u001b[38;5;241m=\u001b[39mtrain_idx, transform\u001b[38;5;241m=\u001b[39mGNNTransformMD(), post_transform\u001b[38;5;241m=\u001b[39mT\u001b[38;5;241m.\u001b[39mRandomTranslate(\u001b[38;5;241m0.05\u001b[39m))\n\u001b[1;32m      3\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mdh5_file' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = ProtDataset(mdh5_file, idx_file=train_idx, transform=GNNTransformMD(), post_transform=T.RandomTranslate(0.05))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, num_workers=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[36840, 11], edge_index=[2, 603016], edge_attr=[603016], y=[36840], pos=[36840, 3], ids=[16], batch=[36840], ptr=[17])\n"
     ]
    }
   ],
   "source": [
    "for idx, val in enumerate(train_loader):\n",
    "    print(val)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mddata = MDDataModule(files_root, h5file=mdh5_file, train=train_idx, val=val_idx, test=test_idx, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mddata.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[38378, 11], edge_index=[2, 622332], edge_attr=[622332], y=[38378], pos=[38378, 3], ids=[16], batch=[38378], ptr=[17])\n"
     ]
    }
   ],
   "source": [
    "train_loader = mddata.train_dataloader()\n",
    "\n",
    "for idx, val in enumerate(train_loader):\n",
    "    print(val)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ccb8a31603cded50fa2c46f04b1e1d1666ffae07322ba5b5b2be24a82f8ce498"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
