{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiSaTo-Dataset: a tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook we will show how our QM and MD dataset are stored in h5 files. We also show how the data can be loaded so that it can be used by a deep learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the useful packages and set up the paths of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    " \n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "sys.path.insert(0, '/p/project/hai_drug_qm/MiSaTo-dataset/src/data/')\n",
    "sys.path.insert(0, '/p/project/hai_drug_qm/MiSaTo-dataset/src/data/components/')\n",
    "sys.path.insert(0, '/p/project/hai_drug_qm/MiSaTo-dataset/data/QM/')\n",
    "\n",
    "from datasets import MolDataset, ProtDataset\n",
    "from transformQM import GNNTransformQM\n",
    "from transformMD import GNNTransformMD\n",
    "from qm_datamodule import QMDataModule\n",
    "from md_datamodule import MDDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file = \"/p/project/hai_drug_qm/MiSaTo-dataset/data/QM/h5_files/qm.hdf5\"\n",
    "norm_file = \"/p/project/hai_drug_qm/MiSaTo-dataset/data/QM/h5_files/qm_norm_fold1.hdf5\"\n",
    "norm_txtfile = \"/p/project/hai_drug_qm/MiSaTo/data/QM/splits/train_norm_fold1.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H5 files presentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the QM H5 file and H5 file used to normalize the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm_H5File = h5py.File(h5_file)\n",
    "qm_normFile = h5py.File(norm_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The molecule names can be accessed using keys method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10gs',\n",
       " '11gs',\n",
       " '13gs',\n",
       " '16pk',\n",
       " '184l',\n",
       " '185l',\n",
       " '186l',\n",
       " '187l',\n",
       " '188l',\n",
       " '1a07']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(qm_H5File.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target values can be accessed by specifiying into bracket the molecule name, then mol_properties and finally the name of the target value that we want to access: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.7383"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm_H5File[\"5gmm\"][\"mol_properties\"][\"Electron_Affinity\"][()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access to the mean and standard-deviation of each target value by specifiying it into bracket.\n",
    "We first specify the set, then the target value and finally either mean or std. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.472403422183977\n",
      "3.811168623042665\n"
     ]
    }
   ],
   "source": [
    "print(qm_normFile[\"train\"][\"Electron_Affinity\"][\"mean\"][()])\n",
    "print(qm_normFile[\"train\"][\"Electron_Affinity\"][\"std\"][()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QM and MD datasets are warped into a PyTorch Dataset class under the name MolDataset and ProtDataset, respectively. \n",
    "The parameters taken by the two classes as well as their types can be found as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MolDataset in module datasets:\n",
      "\n",
      "class MolDataset(torch.utils.data.dataset.Dataset)\n",
      " |  MolDataset(data_file, idx_file, target_norm_file, transform, isTrain=False, post_transform=None)\n",
      " |  \n",
      " |  Load the QM dataset.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MolDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, index: int)\n",
      " |  \n",
      " |  __init__(self, data_file, idx_file, target_norm_file, transform, isTrain=False, post_transform=None)\n",
      " |      Args:\n",
      " |          data_file (str): H5 file path\n",
      " |          idx_file (str): path of txt file which contains pdb ids for a specific split such as train, val or test.\n",
      " |          target_norm_file (str): H5 file path where training mean and std are stored.  \n",
      " |          transform (obj): class that convert a dict to a PyTorch Geometric graph.\n",
      " |          isTrain (bool, optional): Flag to standardize the target values (only used for train set). Defaults to False.\n",
      " |          post_transform (PyTorch Geometric, optional): data augmentation. Defaults to None.\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MolDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ProtDataset in module datasets:\n",
      "\n",
      "class ProtDataset(torch.utils.data.dataset.Dataset)\n",
      " |  ProtDataset(md_data_file, idx_file, transform=None, post_transform=None)\n",
      " |  \n",
      " |  Load the MD dataset\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ProtDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, index: int)\n",
      " |  \n",
      " |  __init__(self, md_data_file, idx_file, transform=None, post_transform=None)\n",
      " |      Args:\n",
      " |          md_data_file (str): H5 file path\n",
      " |          idx_file (str): path of txt file which contains pdb ids for a specific split such as train, val or test.\n",
      " |          transform (obj): class that convert a dict to a PyTorch Geometric graph.\n",
      " |          post_transform (PyTorch Geometric, optional): data augmentation. Defaults to None.\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ProtDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the data by instanciating MolDataset and providing the QM H5 file, the text file that indicates the molecule used for training and the norm file used to normalize the target values. \n",
    "\n",
    "The MolDataset class without any transform return a dictionary that contain the elements and their coordinates. We use GNNTransformQM class to transform our data to a graph that can be used by a GNN. The parameter post_transform is another transformation used to perform data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.RandomTranslate(0.25)\n",
    "batch_size = 128\n",
    "num_workers = 48\n",
    "\n",
    "data_train = MolDataset(h5_file, norm_txtfile, target_norm_file=norm_file, transform=GNNTransformQM(), post_transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can load our data using the PyTorch DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[7679, 25], edge_index=[2, 147188], edge_attr=[147188, 1], y=[256], pos=[7679, 3], id=[128], batch=[7679], ptr=[129])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(data_train, batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "for idx, val in enumerate(train_loader):\n",
    "    print(val)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch lightning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QMDataModule is a class inherated from LightningDataModule that instanciate the MolDataset for training, validation and test set and retrun a dataloader for each set. \n",
    "\n",
    "We start by instanciation of the QMDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_root =  \"/p/project/hai_drug_qm/MiSaTo-dataset/data/QM/\"\n",
    "fold = 1\n",
    "qmdata = QMDataModule(files_root, fold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we call the setup function to instanciate the MolDataset for training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[37, 25], edge_index=[2, 648], edge_attr=[648, 1], y=[2], pos=[37, 3], id='6f23')\n"
     ]
    }
   ],
   "source": [
    "qmdata.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can return a dataloader for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[7847, 25], edge_index=[2, 150976], edge_attr=[150976, 1], y=[256], pos=[7847, 3], id=[128], batch=[7847], ptr=[129])\n"
     ]
    }
   ],
   "source": [
    "train_loader = qmdata.train_dataloader()\n",
    "\n",
    "for idx, val in enumerate(train_loader):\n",
    "    print(val)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MD dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same steps can be used to load the MD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdh5_file = '/p/project/hai_drug_qm/MiSaTo-dataset/data/MD/h5_files/MD_dataset_soft_hard_noH.hdf5'\n",
    "train_idx = \"/p/project/hai_drug_qm/MiSaTo-dataset/data/MD/splits/train_soft_hard.txt\"\n",
    "\n",
    "post_transform = T.RandomTranslate(0.1)\n",
    "\n",
    "train_dataset = ProtDataset(mdh5_file, train_idx, transform=GNNTransformMD(), post_transform=post_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[405300, 11], edge_index=[2, 6424088], edge_attr=[6424088], y=[405300], pos=[405300, 3], ids=[128], batch=[405300], ptr=[129])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "for idx, val in enumerate(train_loader):\n",
    "    print(val)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_root =  \"/p/project/hai_drug_qm/MiSaTo-dataset/data/MD\"\n",
    "\n",
    "mddata = MDDataModule(files_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mddata.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[54188, 11], edge_index=[2, 865460], edge_attr=[865460], y=[54188], pos=[54188, 3], ids=[16], batch=[54188], ptr=[17])\n"
     ]
    }
   ],
   "source": [
    "train_loader = mddata.train_dataloader()\n",
    "\n",
    "for idx, val in enumerate(train_loader):\n",
    "    print(val)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "202424c22d996cd12676b61adff33a676498857df93b61d846a484612cce9839"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
